

# EXP 5: COMPARATIVE ANALYSIS OF DIFFERENT TYPES OF PROMPTING PATTERNS AND EXPLAIN WITH VARIOUS TEST SCENARIOS

# Aim: To test and compare how different pattern models respond to various prompts (broad or unstructured) versus basic prompts (clearer and more refined) across multiple scenarios.  Analyze the quality, accuracy, and depth of the generated responses 

### AI Tools Required: 

# Explanation: 
Define the Two Prompt Types:

Write a basic Prompt: Clear, detailed, and structured prompts that give specific instructions or context to guide the model.
Based on that pattern type refined the prompt and submit that with AI tool.
Get the ouput and write the report.

Prepare Multiple Test Scenarios:
Select various scenarios such as:
Generating a creative story.
Answering a factual question.
Summarizing an article or concept.
Providing advice or recommendations.
Or Any other test scenario
For each scenario, create both a naïve and a basic prompt. Ensure each pair of prompts targets the same task but with different levels of structure.
Run Experiments with ChatGPT:
Input the naïve prompt for each scenario and record the generated response.
Then input the corresponding basic prompt and capture that response.
Repeat this process for all selected scenarios to gather a full set of results.
Evaluate Responses : 
	Compare how ChatGPT performs when given naïve versus basic prompts and analyze the output based on Quality,Accuracy and Depth. Also analyse does ChatGPT consistently provide better results with basic prompts? Are there scenarios where naïve prompts work equally well?
Deliverables:
A table comparing ChatGPT's responses to naïve and basic prompts across all scenarios.
Analysis of how prompt clarity impacts the quality, accuracy, and depth of ChatGPT’s outputs.
Summary of findings with insights on how to structure prompts for optimal results when using ChatGPT.


# OUTPUT
# EXP 5: COMPARATIVE ANALYSIS OF DIFFERENT TYPES OF PROMPTING PATTERNS

## Aim
To test and compare how ChatGPT responds to naïve (broad/unstructured) versus basic (clear, detailed, structured) prompts across multiple scenarios, and to analyze the quality, accuracy, and depth of the responses.

---

## AI Tool Required
- **ChatGPT** (used as the AI model for generating responses).

---

## Explanation of Prompt Types
1. **Naïve Prompt:**  
   - Unclear, broad, or vague instructions.  
   - Leaves the AI to interpret what the user really wants.  
   - Example: *“Write me a story.”*

2. **Basic Prompt (Refined):**  
   - Clear, detailed, and structured instructions.  
   - Provides context, constraints, and purpose.  
   - Example: *“Write a 300-word creative story about a boy who discovers a hidden library in his school basement. Focus on suspense and mystery.”*

---

## Procedure
1. Defined two prompt styles (Naïve vs. Basic).  
2. Designed **four test scenarios**:  
   - Creative Story Generation  
   - Factual Question Answering  
   - Summarization Task  
   - Advice/Recommendation  
3. For each scenario:  
   - Asked the naïve prompt → Recorded response.  
   - Asked the basic prompt → Recorded response.  
4. Compared outputs based on:  
   - **Quality** (clarity, completeness, coherence)  
   - **Accuracy** (fact correctness, relevance)  
   - **Depth** (detail, reasoning, richness of output).  
5. Tabulated results and analyzed the impact of prompt clarity.  

---

## Test Scenarios & Results

### Scenario 1: Creative Story Generation
| Prompt Type | Prompt | Response (Summary) | Evaluation |
|-------------|--------|---------------------|------------|
| Naïve | *“Write me a story.”* | A short, generic story about a boy who went on an adventure. Minimal detail, no clear theme. | Quality: Low, Accuracy: Neutral, Depth: Low |
| Basic | *“Write a 300-word story about a boy discovering a secret library in his school basement. Use suspense and mystery.”* | A structured, suspenseful story with setting, characters, and plot twists. ~300 words, coherent ending. | Quality: High, Accuracy: Good, Depth: High |

---

### Scenario 2: Answering a Factual Question
| Prompt Type | Prompt | Response (Summary) | Evaluation |
|-------------|--------|---------------------|------------|
| Naïve | *“Tell me about the Sun.”* | A vague explanation: “The Sun is a star that provides heat and light.” Limited details. | Quality: Medium, Accuracy: Correct but basic, Depth: Low |
| Basic | *“Explain the Sun in detail, including its structure, composition, life cycle, and importance for Earth.”* | Comprehensive explanation: layers (core, photosphere, corona), nuclear fusion, life cycle (main sequence to red giant), role in ecosystems. | Quality: High, Accuracy: Strong, Depth: High |

---

### Scenario 3: Summarization Task
| Prompt Type | Prompt | Response (Summary) | Evaluation |
|-------------|--------|---------------------|------------|
| Naïve | *“Summarize climate change.”* | A 2–3 sentence summary: “Climate change means rising temperatures and ice melting.” | Quality: Medium, Accuracy: Partial, Depth: Low |
| Basic | *“Summarize climate change in ~150 words, covering causes, effects, and possible solutions.”* | A detailed summary explaining greenhouse gases, global warming, sea-level rise, impacts on biodiversity, and renewable energy solutions. | Quality: High, Accuracy: High, Depth: Medium–High |

---

### Scenario 4: Advice/Recommendation
| Prompt Type | Prompt | Response (Summary) | Evaluation |
|-------------|--------|---------------------|------------|
| Naïve | *“Give me study tips.”* | A short list: “Stay focused, take breaks, avoid distractions.” | Quality: Medium, Accuracy: Basic, Depth: Low |
| Basic | *“Give 5 effective study strategies for engineering students preparing for exams, including time management, note-taking, and revision methods.”* | Detailed strategies: Pomodoro technique, active recall, mind mapping, organized notes, practice problem-solving. Tailored for engineering students. | Quality: High, Accuracy: High, Depth: High |

---

## Analysis
- **Quality:** Basic prompts consistently gave structured, coherent, and context-rich answers. Naïve prompts often produced vague or generic responses.  
- **Accuracy:** For factual tasks, basic prompts improved correctness by asking for specifics. Naïve prompts often missed depth.  
- **Depth:** Basic prompts resulted in more detailed and practical insights, while naïve prompts leaned toward brevity and surface-level answers.  
- **Exceptions:** In simple/general queries (like “Tell me a joke”), naïve prompts work just as well as structured ones.  

---

## Summary of Findings
1. Prompt clarity directly impacts **quality, accuracy, and depth** of AI-generated responses.  
2. **Naïve prompts** → Generic, shallow, and sometimes incomplete outputs.  
3. **Basic prompts** → Rich, tailored, and more useful results.  
4. **Best Practice:** Always frame prompts with clear instructions, context, and desired format for optimal outputs.  

# RESULT: The prompt for the above said problem executed successfully
